# CarrelTeX Progress Ledger

Allowed status enum: `todo | stubbed | implemented | verified | skipped`.
`verified` means the row's proof command is currently green.

Regression guards (proof scenarios):
- `\input{sub.tex}\foo`
- `\input{sub.tex}\meaning\foo`
- `\input{sub.tex}\edef\foo{\bar}\def\bar{A}\foo`
- `\input{sub.tex}\edef\foo{\noexpand\bar}\def\bar{A}\foo`
- `\input{sub.tex}{\xdef\foo{\bar}}\def\bar{A}\foo`
- `\input{sub.tex}\let\bar=\foo\def\foo{A}\bar`
- `\input{sub.tex}\futurelet\bar\noop\foo\bar`
- `\input{sub.tex}\csname foo\endcsname`
- `\input{sub.tex}\string\foo`
- `\input{sub.tex}\expandafter\bar\foo`
- `\input{sub.tex}\ifnum\count0<\count1 XYZ\else AAA\fi`
- `\input{sub.tex}\let\bar=\foo\ifx\bar\foo SAME\else DIFF\fi`

These lock engine order and `\edef`/`\xdef`/`\let` snapshot semantics, plus `\futurelet`/`\csname`/`\string`/`\expandafter` visibility and `\ifnum`/`\ifx` state behavior across input boundaries.

| path | layer | component | status | proof | notes |
| --- | --- | --- | --- | --- | --- |
| `crates/carreltex-core/src/mount.rs` | core | mount-policy | verified | `cargo test --manifest-path crates/carreltex-core/Cargo.toml` | Path policy SSOT via `normalize_path_v0` + `read_file_by_bytes_v0`, resource caps, finalize rules, and byte-level (non-UTF8 allowed) main.tex validation |
| `crates/carreltex-core/src/compile.rs` | core | compile-contract-types-v0 | verified | `cargo test --manifest-path crates/carreltex-core/Cargo.toml` | Compile status/request/result types + canonical report builder/validator + strict TeX stats JSON SSOT (`build_tex_stats_json_v0` + `validate_tex_stats_json_v0`) + status-token/missing-components helper checks + bounded binary event encoding helpers/constants (kind=1 log bytes, kind=2 TeX stats JSON) |
| `crates/carreltex-engine/src/lib.rs` | engine | compile-seam-v0 | verified | `cargo test --manifest-path crates/carreltex-engine/Cargo.toml` | Public engine API remains stable while implementation is modularized into internal submodules; compile behavior now includes tokenizer validation + input expansion v0 (`\\input{path}` and unbraced `\\input path`, where unbraced filename is a non-empty Char run that stops at first `Space`, `BeginGroup`, `EndGroup`, or control sequence; `.` and `-` are accepted as Char bytes, `\\input sub{}` is fail-closed invalid, then paths are normalized via `normalize_path_v0` with `.tex` defaulting before mount lookup and trace logging of resolved paths) + macro expansion v0 (supports `\\def\\foo{body}` plus optional single `Space` token before the body group (`\\def\\foo {body}`), single-parameter `\\def\\foo#1{body}` with optional single `Space` before body group (`\\def\\foo#1 {body}`) and strict braced calls `\\foo{arg}`, `\\edef\\foo{body}` and `\\global\\edef\\foo{body}` with one-time definition-time expansion snapshot semantics, `\\xdef\\foo{body}` and `\\global\\xdef\\foo{body}` as global edef aliases, `\\noexpand` subset that copies the next token without expanding it, `\\ifnum\\countN<op>\\countM ... \\fi` subset for `count0/count1` and operators `<,=,>` with optional single `\\else` at the same nesting level and nesting cap `MAX_IF_DEPTH_V0=64`, and `\\ifx\\foo\\bar ... \\else ... \\fi` subset where operands are control sequences and equality checks binding snapshots without expansion, with nesting cap `MAX_IFX_DEPTH_V0=64`; `\\let` subset `\\let\\a=\\b` / `\\let\\a\\b` with snapshot-at-assignment expansion semantics, `\\futurelet` subset `\\futurelet\\a\\b\\c` where all three are control sequences and `\\a` aliases control-seq literal `\\c` while leaving `\\b\\c` in stream, `\\expandafter` subset `\\expandafter\\a\\b` that deterministically reorders to `\\b\\a`, `\\csname ... \\endcsname` subset where body is non-empty Char-only bytes that become one ControlSeq token, `\\string\\foo` subset that emits Char tokens for literal bytes `\\` + `foo`, `\\meaning\\foo` subset emitting exact ASCII descriptors `macro:<name>` / `alias:<name>-><target>` / `undefined:<name>`, and v0 numeric counters with `\\count0=<digits>` / `\\count1=<digits>` assignments (digits-only, value <= 1_000_000) plus `\\the\\count0` / `\\the\\count1` rendering decimal chars); `\\let` and `\\futurelet` are scope-local like `\\def` while `\\global\\let` and `\\global\\futurelet` write global; `\\def` is group-scoped while `\\gdef`, `\\global\\def`, `\\global\\gdef`, and repeated `\\global` prefixes before `def` or `gdef` write to global scope and can leak across groups; `\\begingroup`/`\\endgroup` and `\\bgroup`/`\\egroup` are translated to group frame tokens and `\\relax` is a no-op token dropped during macro expansion, with `\\endgroup`/`\\egroup` underflow at global scope fail-closed as `macro_group_underflow` and `\\begingroup`/`\\bgroup` synonym depth capped at `MAX_GROUP_DEPTH_V0=1024` via `macro_group_depth_exceeded`; unsupported `\\global` prefix uses fail-closed reason `macro_global_prefix_unsupported`, unsupported `\\xdef` syntax uses `macro_xdef_unsupported`, unsupported `\\noexpand` syntax uses `macro_noexpand_unsupported`, unsupported `\\ifnum` syntax/operator/count uses `macro_ifnum_unsupported`, duplicate else uses `macro_if_else_duplicate`, else without active if uses `macro_if_else_without_if`, missing fi uses `macro_if_missing_fi`, `\\ifnum` depth overflow uses `macro_if_depth_exceeded`, unsupported `\\ifx` syntax/operands use `macro_ifx_unsupported`, duplicate ifx else uses `macro_ifx_else_duplicate`, else without active ifx uses `macro_ifx_else_without_if`, missing ifx fi uses `macro_ifx_missing_fi`, and `\\ifx` depth overflow uses `macro_ifx_depth_exceeded`; unsupported `\\let` targets use `macro_let_unsupported`, unsupported `\\futurelet` syntax uses `macro_futurelet_unsupported`, unsupported `\\expandafter` syntax uses `macro_expandafter_unsupported`, unsupported `\\csname` syntax uses `macro_csname_unsupported`, unsupported `\\string` syntax uses `macro_string_unsupported`, unsupported `\\meaning` syntax uses `macro_meaning_unsupported`, unsupported count assignment syntax uses `macro_count_assignment_unsupported`, and unsupported `\\the` syntax uses `macro_the_unsupported`; other params/`#` forms are fail-closed before parse-stub group-balance and deterministic token stats JSON (events kind=2), with deterministic bounded compile logs and INVALID_INPUT reason-token precedence A-G (request_invalid → mount_finalize_failed → entrypoint_missing → tokenize_failed → input_* → macro_* → stats_build_failed), including `macro_validation_failed` / `macro_params_unsupported` / `macro_cycle_failed` / `macro_depth_exceeded` / `macro_expansions_exceeded`; successful NOT_IMPLEMENTED logs include `INPUT_TRACE_V0:<json>` only when it fully fits max_log_bytes (otherwise omitted, never truncated), with explicit `main.xdv` artifact seam (empty until engine wired) |
| `crates/carreltex-engine/src/tex/tokenize_v0.rs` | engine | tex-tokenizer-v0 | verified | `cargo test --manifest-path crates/carreltex-engine/Cargo.toml` | Deterministic TeX lexing subset with explicit v0 assumptions (NUL invalid, `^^hh` hex byte decode subset with case-insensitive hex digits and unsupported forms fail-closed via `tokenizer_caret_not_supported`, accent control symbols `\\~`/`\\^`/`\\\"` accept only exact raw-braced passthrough form with a single payload token (`Char` byte, or one supported literal control symbol payload from `\\%`/`\\_`/`\\#`/`\\$`/`\\&`/`\\{`/`\\}`/`\\,`) and all other forms fail-closed via `tokenizer_accent_not_supported`, control-sequence bytes must be ASCII-only with fail-closed mapping `tokenizer_control_seq_non_ascii`, `%` comments are consumed raw without caret decoding and terminate at `\\n` or `\\r`, CRLF and lone CR are normalized as one whitespace run, control symbol `\\!` is a v0 tokenizer no-op (drops token, does not swallow following whitespace), control symbols `\\,` and `\\;` map to `Char(' ')` without extra whitespace swallow, control symbol `\\%` maps to `Char('%')` and does not start a comment, control symbol `\\_` maps to `Char('_')`, control symbol `\\#` maps to `Char('#')`, control symbol `\\$` maps to `Char('$')`, control symbol `\\&` maps to `Char('&')`, control symbol `\\{` maps to `Char('{')`, control symbol `\\}` maps to `Char('}')`, control word `\\textbackslash` maps to `Char('\\\\')`, control word `\\textasciitilde` maps to `Char('~')`, control word `\\textasciicircum` maps to `Char('^')`, control word `\\textquotedbl` maps to `Char('\"')`, control words `\\textless`, `\\textgreater`, `\\textbar`, and `\\textendash`/`\\textemdash` map to literal less-than/greater-than/pipe/dash chars, control words `\\textbraceleft` and `\\textbraceright` map to literal brace chars, control words `\\textunderscore`, `\\textquotesingle`, and `\\textasciigrave` map to underscore/single-quote/backtick chars, control words `\\textquotedblleft` and `\\textquotedblright` map to quote chars, exact control word `\\par` maps to a single `Space`, `\\verb` blocked, whitespace coalescing, control words/symbols, token cap fail-closed) |
| `crates/carreltex-wasm-smoke/src/lib.rs` | wasm-adapter | abi-v0 | verified | `./scripts/proof_v0.sh` | Thin ABI adapter over core+engine semantics, strict report/status+missing_components cross-consistency, per-path log bounds + TeX stats JSON invariants with core validator defense-in-depth, deterministic binary events seam carrying kind=1(log bytes)+kind=2(stats JSON), allocator bounded by `MAX_WASM_ALLOC_BYTES_V0` (artifact-aligned), generic artifact-by-name ABI + `main.xdv` copy-out cap enforcement, and mount read-back ABI |
| `scripts/proof_v0.sh` | proof | v0-bundle | verified | `./scripts/proof_v0.sh` | Bundle gate: LOC guard (scans tracked `crates/**/*.rs` + `scripts/**/*.mjs`, hard limit <=1000 lines) + core tests + wasm smoke + ledger check |
| `scripts/wasm_smoke_js_proof.mjs` | proof | wasm-js-smoke | verified | `./scripts/proof_wasm_smoke.sh` | Thin JS proof entrypoint; scenarios are modularized under `scripts/wasm_smoke_js/*.mjs` while preserving proof order/output and ABI compatibility checks including compile-request path |
